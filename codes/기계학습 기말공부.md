
# SVM

## 1.import할 모듈들 넣기


```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns; sns.set()

%matplotlib inline
```

## 2. 랜덤 data 생성


```python
from sklearn.datasets.samples_generator import make_blobs

X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
```




    <matplotlib.collections.PathCollection at 0x1af63258780>




![png](output_4_1.png)


## decision boundary 그리기 - 중간에 있는 선이 가장 좋음


```python
xfit = np.linspace(-1, 3.5)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')

for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]: #기울기와 y절편
    plt.plot(xfit, m * xfit + b, '-k')

plt.xlim(-1, 3.5);
```


![png](output_6_0.png)


마진 높이기가 중요 (margin : 점들과의 거리 중 최단)
## SVM


```python
from sklearn.svm import SVC
model = SVC(kernel='linear', C=1E10) #c가 작을수록 마진 soft
model.fit(X, y)
```




    SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
      kernel='linear', max_iter=-1, probability=False, random_state=None,
      shrinking=True, tol=0.001, verbose=False)



그림 그려주는 함수


```python
def plot_svc_decision_function(model, ax=None, plot_support=True):
    if ax is None:
        ax = plt.gca() #matplotlib의 기본 축들을 그냥 씀
    xlim = ax.get_xlim() # 0~1
    ylim = ax.get_ylim() # 0~1
    
    x = np.linspace(xlim[0], xlim[1], 30) # 0~1 사이의 값 30개
    y = np.linspace(ylim[0], ylim[1], 30)
    
    Y, X = np.meshgrid(y, x)
    xy = np.vstack([X.ravel(), Y.ravel()]).T #ravel - 1차원으로, vstack - 합치기
    P = model.decision_function(xy).reshape(X.shape) 
    
    ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])
    
    if plot_support:
        ax.scatter(model.support_vectors_[:, 0],
                   model.support_vectors_[:, 1],
                   s=300, linewidth=1, facecolors='none')
    ax.set_xlim(xlim)
    ax.set_ylim(ylim)
```


```python
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plot_svc_decision_function(model,plot_support=False);
```


![png](output_11_0.png)


## non-linear deecision boundary


```python
from sklearn.datasets.samples_generator import make_circles
X, y = make_circles(100, factor=.1, noise=.1)

clf = SVC(kernel='linear').fit(X, y)

plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plot_svc_decision_function(clf, plot_support=False);
```


![png](output_13_0.png)



```python
clf = SVC(kernel='rbf', C=1E6)
clf.fit(X, y)
```

    C:\Users\user\Miniconda3\lib\site-packages\sklearn\svm\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
      "avoid this warning.", FutureWarning)
    




    SVC(C=1000000.0, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
      kernel='rbf', max_iter=-1, probability=False, random_state=None,
      shrinking=True, tol=0.001, verbose=False)




```python
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plot_svc_decision_function(clf)
plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],s=300, lw=1, facecolors='none');
```


![png](output_15_0.png)



```python
X, y = make_blobs(n_samples=100, centers=2,
                  random_state=0, cluster_std=1.2)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');
```


![png](output_16_0.png)


# Bias and Variance

## 인공 데이터 생성

$r(x) = f(x) + \epsilon$,

where $\epsilon \sim \mathcal{N}(0,1)$


```python
%matplotlib inline
import numpy as np

def f(size): # 0~4.5사이의 sin값 만들기
    x = np.linspace(0, 4.5, size)
    y = 2 * np.sin(x * 1.5)
    return (x, y)

def sample(size):
    x = np.linspace(0, 4.5, size)
    y = 2 * np.sin(x * 1.5) + np.random.randn(x.size)
    return (x, y)

f_x, f_y = f(50)
plt.plot(f_x, f_y)
x, y = sample(50)
plt.plot(x, y, 'k.')
```




    [<matplotlib.lines.Line2D at 0x1af637342e8>]




![png](output_19_1.png)


## model 만들기

최소자승법 : (Y - pred_Y)^2를 최소로 만들기 (즉 cost를 최소로)


```python
x1 = np.array([1, 2, 3])
print(np.vander(x1, 4))
```

    [[ 1  1  1  1]
     [ 8  4  2  1]
     [27  9  3  1]]
    


```python
from sklearn.linear_model import LinearRegression

def fit_polynomial(x, y, degree):
    model = LinearRegression()
    model.fit(np.vander(x, degree + 1), y)
    return model

def apply_polynomial(model, x):
    degree = model.coef_.size - 1 #계수의 사이즈
    y = model.predict(np.vander(x, degree + 1))
    return y
```


```python
model = fit_polynomial(x, y, 4)
p_y = apply_polynomial(model, x)
plt.plot(f_x, f_y)
plt.plot(x, y, 'k.')
plt.plot(x, p_y)
```




    [<matplotlib.lines.Line2D at 0x1af6372d668>]




![png](output_23_1.png)


## model 학습시키기


```python
degree = 4
n_samples = 40
iteration = 5
avg_y = np.zeros(n_samples)
```


```python
for i in range(iteration):
    (x, y) = sample(n_samples)
    model = fit_polynomial(x, y, degree)
    p_y = apply_polynomial(model, x)
    avg_y = avg_y + p_y
    plt.plot(x, p_y, 'b--')
avg_y = avg_y / iteration
plt.plot(x, avg_y, 'k-')
```




    [<matplotlib.lines.Line2D at 0x1af63747780>]




![png](output_26_1.png)


## bias & variance 계산


```python
from numpy.linalg import norm

n_samples = 40
f_x, f_y = f(n_samples)
iteration = 100
degree = 5

avg_y = np.zeros(n_samples)
for i in range(iteration):
    (x, y) = sample(n_samples)
    model = fit_polynomial(x, y, degree)
    p_y = apply_polynomial(model, x)
    avg_y = avg_y + p_y
avg_y = avg_y / iteration
bias = norm(avg_y - f_y)/f_y.size
print(bias)
#variance ??
```

    0.003763075935108094
    

# K-means

1. 임의의 centroid 값을 기준으로 알고리즘을 실행(시작)
2. 각 데이터 포인트와 centroid 와의 거리를 비교하여 가장 가까운 centroid 찾음
3. centroid 를 업데이트 (주어진 군집에 속한 점들의 평균값을 이용하여 centroid 값 바꿈) 

data 가져오기


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
%matplotlib inline
```

STEP 1


```python
import random as rd

dataset = pd.read_csv('Mall_Customers.csv') #data 가져오기
X = dataset.iloc[:, [3, 4]].values # 3, 4를 씀

m = X.shape[0]
n = X.shape[1]
n_iter = 200

K = 5
Centroids = np.zeros((K, n))

sampled_index = rd.sample(range(m), K)

for i in range(K):
    rand = sampled_index[i]
    Centroids[i] = X[rand]
```

STEP 2


```python
# a
for i in range(n_iter):
    EuclidianDistance = np.zeros((m, K))
    for j in range(m):
        EuclidianDistance[j] = np.sum((X[j] - Centroids[:])**2, axis=1)
        C = np.argmin(EuclidianDistance, axis=1)+1
# b
    Y = {}
    for k in range(K):
        Y[k+1] = []
    for i in range(m):
        Y[C[i]].append(X[i])
```


```python
colors = ['r', 'g', 'b', 'c', 'm']
for i in range(m):
    plt.scatter(X[i][0], X[i][1], s=20, c=colors[C[i]-1])
for k in range(K):
    plt.scatter(Centroids[k][0], Centroids[k][1], s=200, c=colors[k], marker='s')
plt.show()
```


![png](output_37_0.png)


# GMM EM


```python
from scipy.stats import multivariate_normal as mvn
```


```python
def em_gmm_orig(xs, pis, mus, sigmas, tol=0.01, max_iter=1000):
    n, p = xs.shape
    k = len(pis)
    
    
```
